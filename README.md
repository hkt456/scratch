# scratch

This repository contains my implementation of different elements in AI using PyTorch from scratch (as much as possible).

> Currently working on activation functions and machine learning models

## Activation Functions

PyTorch provides several activation functions that you can use in your neural network implementations. Here are some of the commonly used ones:

- **ReLU (Rectified Linear Unit)**: `torch.nn.ReLU`
- **Sigmoid**: `torch.nn.Sigmoid`
- **Tanh (Hyperbolic Tangent)**: `torch.nn.Tanh`
- **Leaky ReLU**: `torch.nn.LeakyReLU`
- **Softmax**: `torch.nn.Softmax`
- **Softplus**: `torch.nn.Softplus`
- **ELU (Exponential Linear Unit)**: `torch.nn.ELU`
- **SELU (Scaled Exponential Linear Unit)**: `torch.nn.SELU`
- **GELU (Gaussian Error Linear Unit)**: `torch.nn.GELU`
- **Swish**: `torch.nn.SiLU` (also known as Sigmoid Linear Unit)

You can find more activation functions and their details in the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity).